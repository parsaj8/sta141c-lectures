---
title: "Profiling R Code"
output: 
  html_document: 
    toc: yes
---

```{r, message = FALSE}
# we will need these packages
library(tidyverse)
library(profvis)
library(bench)
```


# Profiling

[Donald Knuth](https://en.wikipedia.org/wiki/Donald_Knuth) has famously said

> The real problem is that programmers have spent far too much time worrying about efficiency in the wrong places and at the wrong times; premature optimization is the root of all evil (or at least most of it) in programming.

We consider this example of simple linear regression,

```{r}
generate_and_fit <- function(n) {
  x <- runif(n)
  y <- runif(n)
  fit <- lm(y ~ x)
  coef(fit)
}
```

```{r}
generate_and_fit(1e7) #  it is quite slow
```

### `system.time`

The easier tool to do profiling perhaps is the `system.time` function. It reports the amount of time elasped.

```{r}
system.time(generate_and_fit(1e7))
```

See https://en.wikipedia.org/wiki/Time_(Unix)#User_time_vs_system_time for the differences between user and system times. (TLDR, they are not very relevant to us.)

### `Rprof`

`Rprof()` keeps track of the function call stack at regularly sampled intervals and tabulates how much time is spent inside each function.

```{r}
Rprof()
generate_and_fit(1e7)
Rprof(NULL)
result <- summaryRprof()
```

```{r}
result$by.self
```

```{r}
result$by.total
```
It seems that most of the time was spent in `lm.fit`. `help(lm.fit)` says

> These are the basic computing engines called by lm used to fit linear models. These should usually not be used directly unless by experienced users. 


### Visualising profiles


There are two ways to use profvis:

- From the Profile menu in RStudio.

- Use the `profvis` function

```{r}
profvis(generate_and_fit(1e7))
```


### Memory profiling

When an object in R is not referenced by any other objects, it will get GC'ed (garbage collected). If `GC` is taking a lot of time, it’s usually an indication that you’re creating many short-lived objects.

```{r}
# suppose `f` is a function given to you.
n <- 1e4
draw <- sample.int(n, 10)
f <- function(x) x %in% draw
```

```{r}
profvis({
  x <- integer()
  for (i in 1:n) {
    x <- c(x, f(i))
  }
})
```
Each time when `x <- c(x, i)` is execulated, the previous `x` is de-referenced and marked as pending to be GC'ed. R will GC those `x`'s at some point down the line.


So a better approach? In the following code, the vector `x` is pre-allocated.

```{r}
profvis({
  x <- integer(n)
  for (i in 1:n) {
    x[i] <- f(i)
  }
}, 
interval = 0.0005)
```

`compiler` is an R package doing just in time (JIT) compilation.


## Microbenchmark

A microbenchmark is a measurement of the performance of a very small piece of code.

The following code compares the speed of two approaches to computing a square root.

```{r}
x <- runif(100)
bench::mark(
  sqrt(x),
  x^0.5
)
```
```{r}
bench::mark(
  sqrt(x),
  x^0.5,
  relative = TRUE
)
```

`sqrt(x)` is is about 5x faster than `x ^ 0.5`


### Revisit the `map` example.

```{r, paged.print = FALSE}
n <- 1e3
bench::mark(
  `for loop` = {
    x <- integer(n)
    for (i in 1:n) {
      x[i] <- f(i)
    }
    x
  },
  modify = modify(1:n, f),
  map = map_int(1:n, f)
)
```


### A simple linear regression example

We want to compare several ways to perform simple linear regression.

- pure R function
- Rcpp
- `lm`

```{r}
slr <- function(x, y) {
  mux <- mean(x)
  muy <- mean(y)
  sxy <- sum((x - mux) * (y - muy))
  sxx <- sum((x - mux)^2)
  slope <- sxy / sxx
  intercept <- muy - slope * mux
  c(intercept, slope)
}
```

```{Rcpp}
#include <Rcpp.h>
using namespace Rcpp;

// [[Rcpp::export]]
NumericVector slr_cpp(NumericVector x, NumericVector y) {
  double mux = mean(x);
  double muy = mean(y);
  double sxy = sum((x - mux)*(y - muy));
  double sxx = sum(pow(x - mux, 2));
  double slope = sxy / sxx;
  double intercept = muy - slope * mux;
  return NumericVector::create(intercept, slope);
}
```
Remark: No worries, we will cover Rcpp later.


```{r, cached = TRUE}
x <- runif(1e7)
y <- runif(1e7)

(result <- bench::mark(
  slr = slr(x, y),
  slr_cpp = slr_cpp(x, y),
  lm = coef(lm(y ~ x)),
  check = FALSE
))
autoplot(result)
```


There are three levels of collections. 
- level 0 collects only the youngest generation
- level 1 collects the two youngest generations
- level 2 collects all generations.

After 20 level-0 collections the next collection is at level 1, and after 5 level-1 collections at level 2.



# Reference

- Advanced R https://adv-r.hadley.nz/perf-measure.html
- R Programming for Data Science https://bookdown.org/rdpeng/rprogdatascience/profiling-r-code.html


